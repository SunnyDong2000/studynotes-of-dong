{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e11d0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112378e5",
   "metadata": {},
   "source": [
    "torch.Tensor是这个包的核心类。如果设置.requires_grad为True，那么将会追踪所有对于该张量的操作。当完成计算后通过调用.backward()，自动计算所有的梯度，这个张量的所有梯度将会自动积累到.grad属性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad99998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdbdcf6",
   "metadata": {},
   "source": [
    ".requires_grad_( ... )可以改变现有张量的requires_grad属性。如果没有指定的话，默认输入的flag是False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5da65d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor([[-1.4157,  1.4799],\n",
      "        [13.1129,  1.3168]], requires_grad=True)\n",
      "True\n",
      "tensor(177.8756, grad_fn=<SumBackward0>)\n",
      "<SumBackward0 object at 0x000001EDD53A8048>\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b)\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e219251",
   "metadata": {},
   "source": [
    "反向传播因为out是一个scalar,out.backward()等于out.backward(torch.tensor(1))。\n",
    "\n",
    "将out叫做\n",
    "*Tensor* “$o$”.\n",
    "\n",
    "得到 $o = \\frac{1}{4}\\sum_i z_i$,\n",
    "$z_i = 3(x_i+2)^2$ 和 $z_i\\bigr\\rvert_{x_i=1} = 27$.\n",
    "\n",
    "因此,\n",
    "$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, 则\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95783bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c925b5",
   "metadata": {},
   "source": [
    "指定gradient的求导：\n",
    "\n",
    "tensor.backward()在tensor为一个标量时不用指定参数，但在tensor为一个向量、矩阵、张量时，需要指定一个同维的参数gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2b9b188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 1., 7.])\n",
      "tensor([0., 2., 8.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0.0, 2.0, 8.0], requires_grad = True)\n",
    "y = torch.tensor([5.0, 1.0, 7.0], requires_grad = True)\n",
    "z = x * y\n",
    "z.backward(torch.FloatTensor([1.0, 1.0, 1.0]))\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8a1a55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5000, 0.2000, 7.0000])\n",
      "tensor([0.0000, 0.4000, 8.0000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0.0, 2.0, 8.0], requires_grad = True)\n",
    "y = torch.tensor([5.0, 1.0, 7.0], requires_grad = True)\n",
    "z = x * y\n",
    "z.backward(torch.FloatTensor([.1, 0.2, 1.0]))\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df693cf",
   "metadata": {},
   "source": [
    "gradient参数的作用实际上是：\n",
    "\n",
    "先求出Jacobian矩阵中每一个元素的梯度值，然后将这个Jacobian矩阵与gradient参数对应的矩阵进行对应的点乘，得到最终的结果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
