{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf4fea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da92a770",
   "metadata": {},
   "source": [
    "使用pytorch定义网络，2层卷积层、2层池化层、3层全连接层，最终未连接softmax(因为pytorch的交叉熵损失函数内部包含有softmax)，训练时使用单张图像"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fd5c60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        #定义两个卷积层、两个池化层、三个全连接层\n",
    "        #CIFAR-10数据集图像大小为(32,32,3)\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        #conv1输入3个通道(RGB)，有6个卷积核，卷积核大小为5*5\n",
    "        #输出6个通道(每个卷积核一个)，28*28的数据。计算公式：(img_size - kernel_size + 2 * padding)/stride + 1。结果为(32-5)/1 + 1= 28 \n",
    "        #输出(6,28,28)\n",
    "        self.pool = nn.MaxPool2d(2, 2)#2*2最大池化\n",
    "        #第一次池化，输出(6,14,14)。28 / 2 = 14\n",
    "        #第二次池化，输出(16,5,5)。10 / 2 = 5\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        #conv1输入6个通道，有16个卷积核，卷积核大小为5*5\n",
    "        #输出16个通道(每个卷积核一个)，10*10的数据。计算过程为(14-5)/1 + 1= 10\n",
    "        #输出(16,10,10)\n",
    "        \n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120) #全连接层，输入(16,5,5)的数据，输出120个节点\n",
    "        self.fc2 = nn.Linear(120, 84) #全连接层，120个连接84个\n",
    "        self.fc3 = nn.Linear(84, 10) #全连接层，84个连接10个\n",
    "        #\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x))) #第一次卷积、relu激活、池化。输出(6,14,14)的数据\n",
    "        x = self.pool(F.relu(self.conv2(x))) #第二次卷积、relu激活、池化。输出(16,5,5)的数据\n",
    "        x = x.view(-1, x.size()[1:].numel()) #将数据展开成一维\n",
    "        x = F.relu(self.fc1(x)) #第一层全连接、relu激活\n",
    "        x = F.relu(self.fc2(x)) #第二层全连接、relu激活\n",
    "        x = self.fc3(x) #第三层全连接，得到评分\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a651643",
   "metadata": {},
   "source": [
    "从本地文件夹读取数据集，但未对数据进行预处理(均值减法、归一化等)，效果不佳，通过后面的对比可以发现预处理的重要性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b50f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#解码数据集 函数为CIFAR-10提供的python3版本\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "#创建训练集和测试集\n",
    "def CreatData(path):\n",
    "    #依次加载CIFAR-10的5个batch_data,并将其合并为traindata和traindlabels\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for i in range(1,6):\n",
    "        batch_path=path + 'data_batch_%d'%(i) #每个batch的地址\n",
    "        batch_dict=unpickle(batch_path) #解码每个batch\n",
    "        train_batch=batch_dict[b'data'].astype('float') #将每个batch的data部分以float形式存储于train_batch变量\n",
    "        train_labels=np.array(batch_dict[b'labels']) #将每个batch的label部分以np.array的形式存储于train_labels变量\n",
    "        x.append(train_batch)\n",
    "        y.append(train_labels)\n",
    "    #将5个训练样本batch(10000,3072)合并为(50000,3072)，标签合并为(50000,1)\n",
    "    #np.concatenate默认axis=0:按行合并，axis=1则为:按列合并\n",
    "    traindata=torch.from_numpy(np.concatenate(x)).view(-1,3,32,32)\n",
    "    trainlabels=torch.from_numpy(np.concatenate(y)).long().view(-1)\n",
    "    \n",
    "    #加载测试集\n",
    "    testpath=path + 'test_batch' #test_batch的地址\n",
    "    test_dict=unpickle(testpath) #解码test_batch\n",
    "    testdata=torch.from_numpy(test_dict[b'data'].astype('float')).view(-1,3,32,32) #将test_dict的data部分以float形式存储于testdata变量\n",
    "    testlabels=torch.from_numpy(np.array(test_dict[b'labels'])).long().view(-1) #将test_dict的labels部分以np.array形式存储于testlabels变量\n",
    "    \n",
    "    #将训练集数据、训练集标签、测试集数据、测试集标签返回\n",
    "    return traindata,trainlabels,testdata,testlabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9dd9d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#调用自己定义的函数读取本地训练集和测试集的数据和标签\n",
    "traindata,trainlabels,testdata,testlabels = CreatData(\"E:/dataset/cifar-10-batches-py/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e7541e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 , 2000 ] loss: 2.316291594982147\n",
      "[ 1 , 4000 ] loss: 2.3053256458044054\n",
      "[ 1 , 6000 ] loss: 2.3068840577602385\n",
      "[ 1 , 8000 ] loss: 2.3047428538799286\n",
      "[ 1 , 10000 ] loss: 2.305438923239708\n",
      "[ 1 , 12000 ] loss: 2.3055933393239973\n",
      "[ 1 , 14000 ] loss: 2.3055312781333925\n",
      "[ 1 , 16000 ] loss: 2.30385791349411\n",
      "[ 1 , 18000 ] loss: 2.3064646641016004\n",
      "[ 1 , 20000 ] loss: 2.3064027812480927\n",
      "[ 1 , 22000 ] loss: 2.3054550058841707\n",
      "[ 1 , 24000 ] loss: 2.305367486000061\n",
      "[ 1 , 26000 ] loss: 2.3044186067581176\n",
      "[ 1 , 28000 ] loss: 2.303218544483185\n",
      "[ 1 , 30000 ] loss: 2.3044489706754683\n",
      "[ 1 , 32000 ] loss: 2.3029126390218733\n",
      "[ 1 , 34000 ] loss: 2.3044829057455063\n",
      "[ 1 , 36000 ] loss: 2.3064204243421553\n",
      "[ 1 , 38000 ] loss: 2.305007672548294\n",
      "[ 1 , 40000 ] loss: 2.3056939866542816\n",
      "[ 1 , 42000 ] loss: 2.304836077094078\n",
      "[ 1 , 44000 ] loss: 2.306043698310852\n",
      "[ 1 , 46000 ] loss: 2.3044300192594527\n",
      "[ 1 , 48000 ] loss: 2.3045212196111677\n",
      "[ 1 , 50000 ] loss: 2.303801007926464\n",
      "[ 2 , 2000 ] loss: 2.29908312189579\n",
      "[ 2 , 4000 ] loss: 2.3058698305487635\n",
      "[ 2 , 6000 ] loss: 2.305293398141861\n",
      "[ 2 , 8000 ] loss: 2.296674836874008\n",
      "[ 2 , 10000 ] loss: 2.3049191044569017\n",
      "[ 2 , 12000 ] loss: 2.305275116920471\n",
      "[ 2 , 14000 ] loss: 2.3053157991170883\n",
      "[ 2 , 16000 ] loss: 2.303698813080788\n",
      "[ 2 , 18000 ] loss: 2.3062737066745758\n",
      "[ 2 , 20000 ] loss: 2.306209802985191\n",
      "[ 2 , 22000 ] loss: 2.3052524349093435\n",
      "[ 2 , 24000 ] loss: 2.2835188633203507\n",
      "[ 2 , 26000 ] loss: 2.2834320634007454\n",
      "[ 2 , 28000 ] loss: 2.274014178454876\n",
      "[ 2 , 30000 ] loss: 2.3058897830247878\n",
      "[ 2 , 32000 ] loss: 2.288280621945858\n",
      "[ 2 , 34000 ] loss: 2.2817818908691407\n",
      "[ 2 , 36000 ] loss: 2.2860777207612992\n",
      "[ 2 , 38000 ] loss: 2.2892774454951286\n",
      "[ 2 , 40000 ] loss: 2.306102493464947\n",
      "[ 2 , 42000 ] loss: 2.307083296537399\n",
      "[ 2 , 44000 ] loss: 2.3071542282104494\n",
      "[ 2 , 46000 ] loss: 2.304989682674408\n",
      "[ 2 , 48000 ] loss: 2.305370018005371\n",
      "[ 2 , 50000 ] loss: 2.305183034181595\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = Net() #实例化网络\n",
    "criterion = nn.CrossEntropyLoss() #交叉熵损失\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) #构造优化器\n",
    "\n",
    "for epoch in range(2):  # 多批次循环\n",
    "    running_loss = 0.0\n",
    "    for i in range(traindata.shape[0]):\n",
    "        # 获取输入和标签\n",
    "        inputs = traindata[i].unsqueeze(0).float()\n",
    "        labels = trainlabels[i].view(-1)\n",
    "        # 梯度置0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)#正向传播\n",
    "        loss = criterion(outputs, labels)#计算损失\n",
    "        loss.backward()#反向传播\n",
    "        optimizer.step()#更新网络参数\n",
    "\n",
    "        # 打印损失信息\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # 每2000批次打印一次\n",
    "            print('[',epoch + 1,',',i + 1,']','loss:',running_loss / 2000)\n",
    "            running_loss = 0.0\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1345c8da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 10.03 %\n"
     ]
    }
   ],
   "source": [
    "#测试效果\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():#不更新梯度\n",
    "    for i in range(testdata.shape[0]):\n",
    "        # 获取输入\n",
    "        inputs = testdata[i].unsqueeze(0).float()\n",
    "        labels = testlabels[i].view(-1)\n",
    "        \n",
    "        output = net(inputs)#正向传播\n",
    "        _, predicted = torch.max(output, 1)#预测标签\n",
    "        total += labels.size(0)#测试图像数目\n",
    "        correct += (predicted == labels).sum().item()#预测正确数目\n",
    "print('Accuracy of the network on the 10000 test images:',100 * correct / total,'%')\n",
    "#print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fb3b8",
   "metadata": {},
   "source": [
    "使用pytorch的dataset和dataloader，并对数据进行一定的预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a48388d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#读取训练集和测试机数据\n",
    "transform = transforms.Compose([transforms.ToTensor() , transforms.Normalize((0.5, 0.5, 0.5) , (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f46373",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net() #实例化网络\n",
    "criterion = nn.CrossEntropyLoss() #交叉熵损失\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) #构造优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e452ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.210\n",
      "[1,  4000] loss: 1.886\n",
      "[1,  6000] loss: 1.694\n",
      "[1,  8000] loss: 1.592\n",
      "[1, 10000] loss: 1.529\n",
      "[1, 12000] loss: 1.487\n",
      "[2,  2000] loss: 1.404\n",
      "[2,  4000] loss: 1.359\n",
      "[2,  6000] loss: 1.342\n",
      "[2,  8000] loss: 1.328\n",
      "[2, 10000] loss: 1.304\n",
      "[2, 12000] loss: 1.255\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(2):  # 多批次循环\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # 获取输入\n",
    "        inputs, labels = data\n",
    "\n",
    "        # 梯度置0\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)#正向传播\n",
    "        loss = criterion(outputs, labels)#计算损失\n",
    "        loss.backward()#反向传播\n",
    "        optimizer.step()#更新网络参数\n",
    "\n",
    "        # 打印损失信息\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # 每2000批次打印一次\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f158b8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 58 %\n"
     ]
    }
   ],
   "source": [
    "#训练集准确率\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():#不计算梯度\n",
    "    for data in trainloader:\n",
    "        images, labels = data#载入训练数据\n",
    "        outputs = net(images)#正向传播\n",
    "        _, predicted = torch.max(outputs.data, 1)#预测标签\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d86927c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 56 %\n"
     ]
    }
   ],
   "source": [
    "#测试集准确率\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():#不计算梯度\n",
    "    for data in testloader:\n",
    "        images, labels = data#载入训练数据\n",
    "        outputs = net(images)#正向传播\n",
    "        _, predicted = torch.max(outputs.data, 1)#预测标签\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
